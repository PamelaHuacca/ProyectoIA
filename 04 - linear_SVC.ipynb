{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Luiscontreras7/Proyecto-IA-1-/blob/main/04_LinearSVC.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1V_6E9p8HDkz",
        "outputId": "8d39fe85-4eb9-40e0-fc2b-a8c45aebc37d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Warning: Your Kaggle API key is readable by other users on this system! To fix this, you can run 'chmod 600 ./kaggle.json'\n",
            "usage: kaggle [-h] [-v] [-W]\n",
            "              {competitions,c,datasets,d,kernels,k,models,m,files,f,config}\n",
            "              ...\n",
            "kaggle: error: the following arguments are required: command\n"
          ]
        }
      ],
      "source": [
        "\n",
        "import os\n",
        "!kaggle\n",
        "!chmod 600 ./kaggle.json\n",
        "os.environ['KAGGLE_CONFIG_DIR'] = '.'\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1QIDOttuG5Pf",
        "outputId": "9c5d1262-f31d-4a2a-ce2f-b0b8a7e68c3f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading udea-ai-4-eng-20251-pruebas-saber-pro-colombia.zip to /content\n",
            "\r  0% 0.00/29.9M [00:00<?, ?B/s]\n",
            "\r100% 29.9M/29.9M [00:00<00:00, 999MB/s]\n"
          ]
        }
      ],
      "source": [
        "!kaggle competitions download -c udea-ai-4-eng-20251-pruebas-saber-pro-colombia"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BJncLJDLItzC",
        "outputId": "27cb08b3-96cf-4fa7-b6bc-32a5ae2f7b96"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Archive:  udea-ai-4-eng-20251-pruebas-saber-pro-colombia.zip\n",
            "  inflating: submission_example.csv  \n",
            "  inflating: test.csv                \n",
            "  inflating: train.csv               \n"
          ]
        }
      ],
      "source": [
        "!unzip udea-ai-4-eng-20251-pruebas-saber-pro-colombia.zip"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "13vGDyTwKfML"
      },
      "source": [
        "El paradigma a seguir es supervised learning"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jYWLjVXWTczE",
        "outputId": "32b8a4cc-e0aa-4399-d435-4c032f0e1a1f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting unidecode\n",
            "  Downloading Unidecode-1.4.0-py3-none-any.whl.metadata (13 kB)\n",
            "Downloading Unidecode-1.4.0-py3-none-any.whl (235 kB)\n",
            "\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/235.8 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━━━━━━━━━━\u001b[0m \u001b[32m153.6/235.8 kB\u001b[0m \u001b[31m4.4 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m235.8/235.8 kB\u001b[0m \u001b[31m4.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: unidecode\n",
            "Successfully installed unidecode-1.4.0\n"
          ]
        }
      ],
      "source": [
        "!pip install unidecode"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.base import TransformerMixin, BaseEstimator\n",
        "from sklearn.preprocessing import LabelEncoder, OneHotEncoder\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import accuracy_score, classification_report\n",
        "from sklearn.svm import LinearSVC\n",
        "\n",
        "\n",
        "class Preprocessor(BaseEstimator, TransformerMixin):\n",
        "    def __init__(self, config=None, max_features_text=50, auto_detect=False):\n",
        "        \"\"\"\n",
        "        config: optional dict with keys 'binary', 'ordinal', 'onehot', 'label', 'text'\n",
        "        auto_detect: if True, detects column types automatically in fit\n",
        "        \"\"\"\n",
        "        self.config = config or {'binary': [], 'ordinal': {}, 'onehot': [], 'label': [], 'text': []}\n",
        "        self.auto_detect = auto_detect\n",
        "        self.max_features_text = max_features_text\n",
        "        self.mappings_ = {}\n",
        "\n",
        "    def fit(self, X, y=None):\n",
        "        df = X.copy()\n",
        "        if self.auto_detect:\n",
        "            binary, onehot, label, text = [], [], [], []\n",
        "            for col in df.columns:\n",
        "                vals = df[col].dropna().unique()\n",
        "                n_unique = len(vals)\n",
        "                dtype = df[col].dtype\n",
        "                if set(vals).issubset({'Si','No','S','N'}) and n_unique == 2:\n",
        "                    binary.append(col)\n",
        "                elif dtype == object and n_unique > self.max_features_text:\n",
        "                    text.append(col)\n",
        "                elif dtype == object and n_unique <= 10:\n",
        "                    onehot.append(col)\n",
        "                elif dtype == object:\n",
        "                    label.append(col)\n",
        "            self.config = {'binary': binary, 'ordinal': {}, 'onehot': onehot, 'label': label, 'text': text}\n",
        "\n",
        "        self.mappings_['binary'] = {}\n",
        "        for col in self.config.get('binary', []):\n",
        "            vals = df[col].dropna().unique()\n",
        "            self.mappings_['binary'][col] = {'Si': 1, 'No': 0} if 'Si' in vals else {'S': 1, 'N': 0}\n",
        "\n",
        "        self.mappings_['ordinal'] = {}\n",
        "        for col, order in self.config.get('ordinal', {}).items():\n",
        "            mapping = {v: i for i, v in enumerate(order)} if order else {v: i for i, v in enumerate(sorted(df[col].dropna().unique()))}\n",
        "            self.mappings_['ordinal'][col] = mapping\n",
        "\n",
        "        self.ohe_ = {}\n",
        "        for col in self.config.get('onehot', []):\n",
        "            ohe = OneHotEncoder(sparse_output=False, handle_unknown='ignore')\n",
        "            ohe.fit(df[[col]].astype(str))\n",
        "            self.ohe_[col] = ohe\n",
        "\n",
        "        self.le_ = {}\n",
        "        for col in self.config.get('label', []):\n",
        "            le = LabelEncoder()\n",
        "            le.fit(df[col].dropna().astype(str))\n",
        "            self.le_[col] = le\n",
        "\n",
        "        self.tfidf_ = {}\n",
        "        for col in self.config.get('text', []):\n",
        "            tfidf = TfidfVectorizer(max_features=self.max_features_text, lowercase=True, ngram_range=(1,2), min_df=2, max_df=0.95)\n",
        "            tfidf.fit(df[col].fillna('').astype(str))\n",
        "            self.tfidf_[col] = tfidf\n",
        "\n",
        "        return self\n",
        "\n",
        "    def transform(self, X):\n",
        "        df = X.copy()\n",
        "        for col, m in self.mappings_.get('binary', {}).items():\n",
        "            df[col] = df[col].map(m).fillna(0).astype(int)\n",
        "        for col, m in self.mappings_.get('ordinal', {}).items():\n",
        "            df[col] = df[col].map(m).fillna(-1).astype(int)\n",
        "        for col, ohe in self.ohe_.items():\n",
        "            arr = ohe.transform(df[[col]].astype(str))\n",
        "            cols = [f\"{col}_oh_{cat}\" for cat in ohe.categories_[0]]\n",
        "            df_oh = pd.DataFrame(arr, columns=cols, index=df.index)\n",
        "            df = pd.concat([df, df_oh], axis=1)\n",
        "            df.drop(columns=[col], inplace=True)\n",
        "        for col, le in self.le_.items():\n",
        "            df[col] = df[col].fillna('').apply(lambda x: le.transform([x])[0] if x in le.classes_ else -1)\n",
        "        for col, tfidf in self.tfidf_.items():\n",
        "            arr = tfidf.transform(df[col].fillna('').astype(str)).toarray()\n",
        "            cols = [f\"{col}_tfidf_{i}\" for i in range(arr.shape[1])]\n",
        "            df_t = pd.DataFrame(arr, columns=cols, index=df.index)\n",
        "            df = pd.concat([df, df_t], axis=1)\n",
        "            df.drop(columns=[col], inplace=True)\n",
        "        for col in df.columns:\n",
        "            if df[col].isnull().any():\n",
        "                vals = df[col].dropna().values\n",
        "                df.loc[df[col].isnull(), col] = np.random.choice(vals, size=df[col].isnull().sum())\n",
        "        return df\n",
        "\n",
        "\n",
        "def load_data(path):\n",
        "    df = pd.read_csv(path)\n",
        "    print(f\"Loaded {path} with shape {df.shape}\")\n",
        "    return df\n",
        "\n",
        "\n",
        "def full_pipeline(train_path, test_path, id_col, target_col, config=None,\n",
        "                  auto_detect=False, sample_frac=None, output_submission=None):\n",
        "    \"\"\"\n",
        "    Train and evaluate using Logistic Regression\n",
        "    \"\"\"\n",
        "    train = load_data(train_path)\n",
        "    test = load_data(test_path)\n",
        "\n",
        "    if sample_frac:\n",
        "        train = train.sample(frac=sample_frac, random_state=42).reset_index(drop=True)\n",
        "        test = test.sample(frac=sample_frac, random_state=42).reset_index(drop=True)\n",
        "\n",
        "    source = [c for c in train.columns if c not in [id_col, target_col]]\n",
        "    all_data = pd.concat([train[source], test[source]], ignore_index=True)\n",
        "\n",
        "    pre = Preprocessor(config=config, auto_detect=auto_detect)\n",
        "    pre.fit(all_data)\n",
        "    prep = pre.transform(all_data)\n",
        "\n",
        "    n = len(train)\n",
        "    X_train, y_train = prep.iloc[:n].values, train[target_col].values\n",
        "    X_test = prep.iloc[n:].values\n",
        "\n",
        "    model = LogisticRegression(max_iter=1000)\n",
        "    model.fit(X_train, y_train)\n",
        "    print(\"Logistic Regression accuracy:\", accuracy_score(y_train, model.predict(X_train)))\n",
        "\n",
        "    preds = model.predict(X_test)\n",
        "    sub = pd.DataFrame({id_col: test[id_col], target_col: preds})\n",
        "    if output_submission: sub.to_csv(output_submission, index=False)\n",
        "    return sub, model\n",
        "\n",
        "\n",
        "def full_pipeline_svm(train_path, test_path, id_col, target_col, config=None,\n",
        "                      auto_detect=False, sample_frac=None, output_submission=None):\n",
        "    \"\"\"\n",
        "    Train and evaluate using LinearSVC (fast for large datasets).\n",
        "    Hyperparámetros ajustados para velocidad y rendimiento.\n",
        "    \"\"\"\n",
        "    train = load_data(train_path)\n",
        "    test = load_data(test_path)\n",
        "\n",
        "    if sample_frac:\n",
        "        train = train.sample(frac=sample_frac, random_state=42).reset_index(drop=True)\n",
        "        test = test.sample(frac=sample_frac, random_state=42).reset_index(drop=True)\n",
        "\n",
        "    source = [c for c in train.columns if c not in [id_col, target_col]]\n",
        "    all_data = pd.concat([train[source], test[source]], ignore_index=True)\n",
        "\n",
        "    pre = Preprocessor(config=config, auto_detect=auto_detect)\n",
        "    pre.fit(all_data)\n",
        "    prep = pre.transform(all_data)\n",
        "\n",
        "    n = len(train)\n",
        "    X_train, y_train = prep.iloc[:n].values, train[target_col].values\n",
        "    X_test = prep.iloc[n:].values\n",
        "\n",
        "    svm = LinearSVC(C=0.5, max_iter=2000, dual=False)\n",
        "    svm.fit(X_train, y_train)\n",
        "    train_pred = svm.predict(X_train)\n",
        "    print(\"LinearSVC train accuracy:\", accuracy_score(y_train, train_pred))\n",
        "    print(classification_report(y_train, train_pred))\n",
        "\n",
        "    preds = svm.predict(X_test)\n",
        "    sub = pd.DataFrame({id_col: test[id_col], target_col: preds})\n",
        "    if output_submission: sub.to_csv(output_submission, index=False)\n",
        "    return sub, svm\n"
      ],
      "metadata": {
        "id": "5r1TIggi6OkR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "sub_svm, model_svm = full_pipeline_svm(\n",
        "    'train.csv', 'test.csv',\n",
        "    id_col='ID', target_col='RENDIMIENTO_GLOBAL',\n",
        "    auto_detect=True, sample_frac=0.1,\n",
        "    output_submission='submission_svm.csv'\n",
        ")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YPAWEg3Z6UF5",
        "outputId": "e391d96c-5432-4fcf-9847-46139c264fda"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loaded train.csv with shape (692500, 21)\n",
            "Loaded test.csv with shape (296786, 20)\n",
            "LinearSVC train accuracy: 0.2927797833935018\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "        alto       0.32      0.54      0.40     17645\n",
            "        bajo       0.28      0.24      0.26     17174\n",
            "  medio-alto       0.10      0.00      0.00     17198\n",
            "  medio-bajo       0.27      0.38      0.32     17233\n",
            "\n",
            "    accuracy                           0.29     69250\n",
            "   macro avg       0.24      0.29      0.24     69250\n",
            "weighted avg       0.24      0.29      0.24     69250\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lC_1SW3gZMgw"
      },
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}